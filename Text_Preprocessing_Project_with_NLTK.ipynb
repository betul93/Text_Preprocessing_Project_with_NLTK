{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization, Cleaning, Stemming, and Lemmatization**"
      ],
      "metadata": {
        "id": "1-5YGz3JREmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data quietly\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Initialize tools\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text, fallback_split=True):\n",
        "    \"\"\"\n",
        "    Advanced text preprocessing function.\n",
        "    Includes tokenization, lowercasing, punctuation removal, stopword removal, stemming, and lemmatization.\n",
        "    \"\"\"\n",
        "    # Tokenization\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "    except LookupError:\n",
        "        print(\"Punkt tokenizer not found, using simple split() instead.\")\n",
        "        tokens = text.split() if fallback_split else []\n",
        "\n",
        "    # Lowercasing\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Punctuation cleaning\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "\n",
        "    # Keep only alphabetic and non-empty words\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmed = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Results\n",
        "    return {\n",
        "        'original_tokens': tokens,\n",
        "        'cleaned_words': words,\n",
        "        'stemmed': stemmed,\n",
        "        'lemmatized': lemmatized\n",
        "    }\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"NLTK (Natural Language Toolkit) is developed by the University of Pennsylvania and MIT researchers\n",
        "         including Steven Bird and Edward Loper. It's widely used in New York, London, and Tokyo for\n",
        "         processing English, Spanish, and Japanese text. Apple and Google have also contributed to its development.\"\"\"\n",
        "\n",
        "# Process the text\n",
        "results = preprocess_text(text)\n",
        "\n",
        "# Print results\n",
        "print(\"‚úÖ Original Tokens:\", results['original_tokens'])\n",
        "print(\"‚úÖ Cleaned Words:\", results['cleaned_words'])\n",
        "print(\"‚úÖ Stemmed Words:\", results['stemmed'])\n",
        "print(\"‚úÖ Lemmatized Words:\", results['lemmatized'])\n"
      ],
      "metadata": {
        "id": "--zWf3fbmCWu",
        "outputId": "0636290d-f9cc-4e82-d77c-e97c2795b72c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punkt tokenizer not found, using simple split() instead.\n",
            "‚úÖ Original Tokens: ['nltk', '(natural', 'language', 'toolkit)', 'is', 'developed', 'by', 'the', 'university', 'of', 'pennsylvania', 'and', 'mit', 'researchers', 'including', 'steven', 'bird', 'and', 'edward', 'loper.', \"it's\", 'widely', 'used', 'in', 'new', 'york,', 'london,', 'and', 'tokyo', 'for', 'processing', 'english,', 'spanish,', 'and', 'japanese', 'text.', 'apple', 'and', 'google', 'have', 'also', 'contributed', 'to', 'its', 'development.']\n",
            "‚úÖ Cleaned Words: ['nltk', 'natural', 'language', 'toolkit', 'developed', 'university', 'pennsylvania', 'mit', 'researchers', 'including', 'steven', 'bird', 'edward', 'loper', 'widely', 'used', 'new', 'york', 'london', 'tokyo', 'processing', 'english', 'spanish', 'japanese', 'text', 'apple', 'google', 'also', 'contributed', 'development']\n",
            "‚úÖ Stemmed Words: ['nltk', 'natur', 'languag', 'toolkit', 'develop', 'univers', 'pennsylvania', 'mit', 'research', 'includ', 'steven', 'bird', 'edward', 'loper', 'wide', 'use', 'new', 'york', 'london', 'tokyo', 'process', 'english', 'spanish', 'japanes', 'text', 'appl', 'googl', 'also', 'contribut', 'develop']\n",
            "‚úÖ Lemmatized Words: ['nltk', 'natural', 'language', 'toolkit', 'developed', 'university', 'pennsylvania', 'mit', 'researcher', 'including', 'steven', 'bird', 'edward', 'loper', 'widely', 'used', 'new', 'york', 'london', 'tokyo', 'processing', 'english', 'spanish', 'japanese', 'text', 'apple', 'google', 'also', 'contributed', 'development']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS Tagging (Part-of-Speech Tagging)**"
      ],
      "metadata": {
        "id": "PCPrZ4qnzMoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwdV31iLzIE0",
        "outputId": "195fb30e-d40b-41b6-96a1-502aab6b2762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "# POS Tagging\n",
        "pos_tags = pos_tag(results['original_tokens'])\n",
        "\n",
        "print(\"‚úÖ POS Tagged Tokens:\")\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word} ‚Üí {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y907M_mcAHd",
        "outputId": "3323d61e-4212-4c06-e711-d266f1958ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ POS Tagged Tokens:\n",
            "nltk ‚Üí JJ\n",
            "(natural ‚Üí JJ\n",
            "language ‚Üí NN\n",
            "toolkit) ‚Üí NN\n",
            "is ‚Üí VBZ\n",
            "developed ‚Üí VBN\n",
            "by ‚Üí IN\n",
            "the ‚Üí DT\n",
            "university ‚Üí NN\n",
            "of ‚Üí IN\n",
            "pennsylvania ‚Üí NN\n",
            "and ‚Üí CC\n",
            "mit ‚Üí NN\n",
            "researchers ‚Üí NNS\n",
            "including ‚Üí VBG\n",
            "steven ‚Üí JJ\n",
            "bird ‚Üí NN\n",
            "and ‚Üí CC\n",
            "edward ‚Üí NN\n",
            "loper. ‚Üí NN\n",
            "it's ‚Üí NN\n",
            "widely ‚Üí RB\n",
            "used ‚Üí VBN\n",
            "in ‚Üí IN\n",
            "new ‚Üí JJ\n",
            "york, ‚Üí NN\n",
            "london, ‚Üí NN\n",
            "and ‚Üí CC\n",
            "tokyo ‚Üí NN\n",
            "for ‚Üí IN\n",
            "processing ‚Üí VBG\n",
            "english, ‚Üí JJ\n",
            "spanish, ‚Üí NN\n",
            "and ‚Üí CC\n",
            "japanese ‚Üí JJ\n",
            "text. ‚Üí NN\n",
            "apple ‚Üí NN\n",
            "and ‚Üí CC\n",
            "google ‚Üí NN\n",
            "have ‚Üí VBP\n",
            "also ‚Üí RB\n",
            "contributed ‚Üí VBN\n",
            "to ‚Üí TO\n",
            "its ‚Üí PRP$\n",
            "development. ‚Üí NN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS Tag Filtering**\n",
        "\n",
        "**Filtering Only Nouns (NN), Verbs (VB), and Adjectives (JJ)**"
      ],
      "metadata": {
        "id": "4FpKcTrg4EUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Allowed POS tag prefixes\n",
        "allowed_tags_prefixes = ('NN', 'VB', 'JJ')\n",
        "\n",
        "# Filtering\n",
        "filtered_pos = [(word, tag) for word, tag in pos_tags if tag.startswith(allowed_tags_prefixes)]\n",
        "\n",
        "# Extract only words\n",
        "filtered_words = [word for word, tag in filtered_pos]\n",
        "\n",
        "print(\"üîç Filtered (Noun/Verb/Adj) words:\")\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xlfjqes1Nkr",
        "outputId": "81412255-1f10-423f-b7f4-659c22d470a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Filtered (Noun/Verb/Adj) words:\n",
            "['nltk', '(natural', 'language', 'toolkit)', 'is', 'developed', 'university', 'pennsylvania', 'mit', 'researchers', 'including', 'steven', 'bird', 'edward', 'loper.', \"it's\", 'used', 'new', 'york,', 'london,', 'tokyo', 'processing', 'english,', 'spanish,', 'japanese', 'text.', 'apple', 'google', 'have', 'contributed', 'development.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# **NER:Named Entity Recognition**\n",
        "\n",
        "Automatically identifies and labels named entities in a text.\n",
        "Person names ‚Äî PERSON\n",
        "ORG ‚Äî Organizations\n",
        "GPE ‚Äî Geopolitical entities (countries, cities)\n",
        "DATE ‚Äî Dates\n",
        "TIME ‚Äî Times\n",
        "MONEY ‚Äî Monetary amounts\n",
        "PRODUCT ‚Äî Product names\n",
        "LAW ‚Äî Laws/regulations\n",
        "EVENT ‚Äî Events\n",
        "\n",
        "The spaCy library was used."
      ],
      "metadata": {
        "id": "ohXRYi9OLkfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"‚úÖ spaCy Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ‚Üí {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ACasFXqSrWe",
        "outputId": "d39e0aa9-b45b-40ed-fa26-1809ea767a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ spaCy Named Entities:\n",
            "the University of Pennsylvania ‚Üí ORG\n",
            "MIT ‚Üí ORG\n",
            "Steven Bird ‚Üí PERSON\n",
            "Edward Loper ‚Üí PERSON\n",
            "New York ‚Üí GPE\n",
            "London ‚Üí GPE\n",
            "Tokyo ‚Üí GPE\n",
            "English ‚Üí LANGUAGE\n",
            "Spanish ‚Üí NORP\n",
            "Japanese ‚Üí NORP\n",
            "Apple ‚Üí ORG\n",
            "Google ‚Üí ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Chunking (Grammatical Phrases)**\n",
        "\n",
        "Let's extract the sentence structure: subject, verb phrase, noun phrase, etc."
      ],
      "metadata": {
        "id": "NKYz3O-3dCFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"‚úÖ Noun Phrases (spaCy):\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(f\"NP ‚Üí {chunk.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7F_hE02dIdF",
        "outputId": "d9498883-f6c8-4116-c52f-2b310793288e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Noun Phrases (spaCy):\n",
            "NP ‚Üí NLTK\n",
            "NP ‚Üí (Natural Language Toolkit\n",
            "NP ‚Üí Pennsylvania\n",
            "NP ‚Üí MIT\n",
            "NP ‚Üí Steven Bird\n",
            "NP ‚Üí Edward Loper\n",
            "NP ‚Üí It\n",
            "NP ‚Üí New York\n",
            "NP ‚Üí London\n",
            "NP ‚Üí Tokyo\n",
            "NP ‚Üí English\n",
            "NP ‚Üí Spanish\n",
            "NP ‚Üí Japanese text\n",
            "NP ‚Üí Apple\n",
            "NP ‚Üí Google\n",
            "NP ‚Üí its development\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Text Mining / Meaning Extraction Section.**\n",
        "Here, we extract statistical information from texts.**\n"
      ],
      "metadata": {
        "id": "Ig3nkkUQaTHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- TF-IDF stands for \"Term Frequency ‚Äì Inverse Document Frequency.\"**\n",
        "\n",
        "This method is used to quantitatively measure how important a word is within a document.\n",
        "It is especially common in text mining and information extraction.\n",
        "\n",
        "Why is it used?\n",
        "\n",
        "To find keywords\n",
        "\n",
        "To create vectors before text classification\n",
        "\n",
        "To rank in search engines\n",
        "\n",
        "Feature extraction in machine learning models\n"
      ],
      "metadata": {
        "id": "K9IQac41dO7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Join cleaned words\n",
        "cleaned_text = \" \".join(results['cleaned_words'])\n",
        "\n",
        "# Create single-document list (TF-IDF expects this structure)\n",
        "documents = [cleaned_text]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get TF-IDF scores\n",
        "tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "\n",
        "# Print top 5 words with highest scores\n",
        "top_scores = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "print(\"üî∑ Top 5 Important Words (TF-IDF):\")\n",
        "for word, score in top_scores:\n",
        "    print(f\"{word} ‚Üí {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImmQ9jLbdaD6",
        "outputId": "fbc9d139-518f-4c83-f4ae-23ffdae5f9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî∑ Top 5 Important Words (TF-IDF):\n",
            "also ‚Üí 0.1826\n",
            "apple ‚Üí 0.1826\n",
            "bird ‚Üí 0.1826\n",
            "contributed ‚Üí 0.1826\n",
            "developed ‚Üí 0.1826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2-) N-gram Analysis (Bigram / Trigram)**\n",
        "\n",
        "We will find the most frequent 2-word (bigram) or 3-word (trigram) word groups."
      ],
      "metadata": {
        "id": "MHEzLpcTaoR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "# Get preprocessed words\n",
        "clean_words = results['cleaned_words']  # tokenized and cleaned words\n",
        "\n",
        "# Bigram (2-word groups)\n",
        "bigrams = list(ngrams(clean_words, 2))\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "# Trigram (3-word groups)\n",
        "trigrams = list(ngrams(clean_words, 3))\n",
        "trigram_counts = Counter(trigrams)\n",
        "\n",
        "# Print top 5 most frequent bigrams and trigrams\n",
        "print(\"üîπ Top 5 Most Frequent Bigrams:\")\n",
        "for bg, count in bigram_counts.most_common(5):\n",
        "    print(f\"{bg} ‚Üí {count} times\")\n",
        "\n",
        "print(\"\\nüîπ Top 5 Most Frequent Trigrams:\")\n",
        "for tg, count in trigram_counts.most_common(5):\n",
        "    print(f\"{tg} ‚Üí {count} times\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04ZXeKJUast1",
        "outputId": "df4fd6f9-252b-4846-b6e5-38eff7c6ff0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Top 5 Most Frequent Bigrams:\n",
            "('nltk', 'natural') ‚Üí 1 times\n",
            "('natural', 'language') ‚Üí 1 times\n",
            "('language', 'toolkit') ‚Üí 1 times\n",
            "('toolkit', 'developed') ‚Üí 1 times\n",
            "('developed', 'university') ‚Üí 1 times\n",
            "\n",
            "üîπ Top 5 Most Frequent Trigrams:\n",
            "('nltk', 'natural', 'language') ‚Üí 1 times\n",
            "('natural', 'language', 'toolkit') ‚Üí 1 times\n",
            "('language', 'toolkit', 'developed') ‚Üí 1 times\n",
            "('toolkit', 'developed', 'university') ‚Üí 1 times\n",
            "('developed', 'university', 'pennsylvania') ‚Üí 1 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3-Word Frequency:**\n",
        "\n",
        "Topic Detection: The most frequent words indicate the main theme of the text.\n",
        "\n",
        "‚úÖ Preliminary Analysis: Useful for getting an initial understanding of the dataset.\n",
        "\n",
        "‚úÖ Feature Extraction: Frequently occurring words can be used as features in machine learning models.\n",
        "\n",
        "‚úÖ Identifying Unnecessary Words: Helps filter out overly repeated but meaningless words."
      ],
      "metadata": {
        "id": "cPZ2-zzNfHsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Cleaned words\n",
        "clean_words = results['cleaned_words']\n",
        "\n",
        "# Calculate word frequency\n",
        "word_counts = Counter(clean_words)\n",
        "\n",
        "# Get top 10 most frequent words\n",
        "most_common_words = word_counts.most_common(10)\n",
        "\n",
        "print(\"üî∏ Top 10 Most Used Words:\")\n",
        "for word, count in most_common_words:\n",
        "    print(f\"{word} ‚Üí {count} times\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D42zLQ_yfMuR",
        "outputId": "52d7daae-40a4-4194-e7e3-06bf067bd9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî∏ Top 10 Most Used Words:\n",
            "nltk ‚Üí 1 times\n",
            "natural ‚Üí 1 times\n",
            "language ‚Üí 1 times\n",
            "toolkit ‚Üí 1 times\n",
            "developed ‚Üí 1 times\n",
            "university ‚Üí 1 times\n",
            "pennsylvania ‚Üí 1 times\n",
            "mit ‚Üí 1 times\n",
            "researchers ‚Üí 1 times\n",
            "including ‚Üí 1 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4-Concordance:**\n",
        "\n",
        "Shows the occurrences of a word within the text, along with a few words before and after it.\n",
        "This way, you can understand:\n",
        "\n",
        "-In what sense is the word used?\n",
        "\n",
        "-In which contexts does it appear?\n",
        "\n",
        "-What are the differences in discourse style, tone, and usage?\n",
        "\n"
      ],
      "metadata": {
        "id": "lynNR4-if_6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import Text\n",
        "\n",
        "# 1. Use the previously tokenized original words\n",
        "original_tokens = results['original_tokens']\n",
        "\n",
        "# 2. Convert to nltk Text object\n",
        "nltk_text = Text(original_tokens)\n",
        "\n",
        "# 3. Specify the target word\n",
        "target_word = 'and'  # change this to any word you want\n",
        "\n",
        "# 4. Print concordance output\n",
        "print(f\"üîç Occurrences of the word '{target_word}' in context:\")\n",
        "nltk_text.concordance(target_word, width=80, lines=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwOzOodygIou",
        "outputId": "1632f9b3-8cb0-4e6c-da5d-a3fe5c73a669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Occurrences of the word 'and' in context:\n",
            "Displaying 5 of 5 matches:\n",
            "ped by the university of pennsylvania and mit researchers including steven bird\n",
            "mit researchers including steven bird and edward loper. it's widely used in new\n",
            "it's widely used in new york, london, and tokyo for processing english, spanish\n",
            "okyo for processing english, spanish, and japanese text. apple and google have \n",
            "sh, spanish, and japanese text. apple and google have also contributed to its d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Classification and NLP Models**"
      ],
      "metadata": {
        "id": "wJladhCclwPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-Sentiment Analysis**\n",
        "\n",
        "I used NLTK‚Äôs built-in sentiment analysis tool called VADER (Valence Aware Dictionary and sEntiment Reasoner)."
      ],
      "metadata": {
        "id": "uaWCk0snl_Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download required data (once)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Example: join cleaned text\n",
        "clean_text = \" \".join(results['cleaned_words'])\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Get sentiment scores\n",
        "sentiment_scores = sia.polarity_scores(clean_text)\n",
        "\n",
        "print(\"Sentiment Scores:\", sentiment_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjoTZw1SmC4p",
        "outputId": "3bb2ce84-30ab-4730-9688-fe532fd52de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.921, 'pos': 0.079, 'compound': 0.3612}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2-Text Classification**"
      ],
      "metadata": {
        "id": "llUsNAxfr6VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created the training data ourselves.."
      ],
      "metadata": {
        "id": "XQdK2DiZvhGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF + Naive Bayes + Classification Model with simple split()"
      ],
      "metadata": {
        "id": "HjlXB8NVveHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Tools\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# üîß Simple split() preprocessing\n",
        "def preprocess_text(text):\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(stemmer.stem(t)) for t in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# üìù Training data\n",
        "texts = [\n",
        "    \"NLTK (Natural Language Toolkit) is developed by the University of Pennsylvania and MIT researchers.\",\n",
        "    \"I love this product, it is amazing and works perfectly.\",\n",
        "    \"This is the worst experience I've ever had.\",\n",
        "    \"Apple released a new iPhone model.\",\n",
        "    \"The football match was thrilling and exciting.\",\n",
        "    \"Google is investing in AI technology.\",\n",
        "    \"I hate waiting in long queues.\"\n",
        "]\n",
        "\n",
        "labels = [\n",
        "    \"tech\",       # technology\n",
        "    \"positive\",   # positive sentiment\n",
        "    \"negative\",   # negative sentiment\n",
        "    \"tech\",\n",
        "    \"sports\",\n",
        "    \"tech\",\n",
        "    \"negative\"\n",
        "]\n",
        "\n",
        "# ‚úÖ Clean texts\n",
        "clean_texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"SVM\": SVC(probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
        "}\n",
        "\n",
        "# üîç Test text\n",
        "test_text = \"\"\"NLTK (Natural Language Toolkit) is developed by the University of Pennsylvania and MIT researchers\n",
        "including Steven Bird and Edward Loper. It's widely used in New York, London, and Tokyo for\n",
        "processing English, Spanish, and Japanese text. Apple and Google have also contributed to its development.\"\"\"\n",
        "\n",
        "test_clean = preprocess_text(test_text)\n",
        "\n",
        "print(f\"\\nüìÑ Test Text:\\n{test_text}\\n\")\n",
        "\n",
        "# Train and predict with each model\n",
        "for name, clf in models.items():\n",
        "    pipeline = make_pipeline(TfidfVectorizer(), clf)\n",
        "    pipeline.fit(clean_texts, labels)\n",
        "    prediction = pipeline.predict([test_clean])\n",
        "    print(f\"üîç {name} Prediction: **{prediction[0]}**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rNw4KVP0Jb9",
        "outputId": "d4abc914-5aec-49b4-fa3d-5ab424e64484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ Test Text:\n",
            "NLTK (Natural Language Toolkit) is developed by the University of Pennsylvania and MIT researchers\n",
            "including Steven Bird and Edward Loper. It's widely used in New York, London, and Tokyo for\n",
            "processing English, Spanish, and Japanese text. Apple and Google have also contributed to its development.\n",
            "\n",
            "üîç Naive Bayes Prediction: **tech**\n",
            "üîç Decision Tree Prediction: **tech**\n",
            "üîç SVM Prediction: **tech**\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Random Forest Prediction: **tech**\n",
            "üîç Logistic Regression Prediction: **tech**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Language Modeling and Lexicons**"
      ],
      "metadata": {
        "id": "U9h7cJ9dadNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-WordNet Integration: Synonyms, Antonyms, Hierarchical Relationships**"
      ],
      "metadata": {
        "id": "sLQPA6myai4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "\n",
        "nltk.download('omw-1.4')  # Required for WordNet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example words (from your text)\n",
        "words_to_check = ['develop', 'technology', 'research', 'language']\n",
        "\n",
        "def explore_wordnet(word):\n",
        "    print(f\"\\nüîç Word: {word}\")\n",
        "\n",
        "    synsets = wn.synsets(word)\n",
        "    if not synsets:\n",
        "        print(\"‚ùó Word not found in WordNet.\")\n",
        "        return\n",
        "\n",
        "    # üîπ 1. Synonyms\n",
        "    synonyms = set()\n",
        "    for syn in synsets:\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    print(f\"üìò Synonyms: {', '.join(synonyms)}\")\n",
        "\n",
        "    # üî∏ 2. Antonyms\n",
        "    antonyms = set()\n",
        "    for syn in synsets:\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.antonyms():\n",
        "                antonyms.add(lemma.antonyms()[0].name())\n",
        "    print(f\"üìï Antonyms: {', '.join(antonyms) if antonyms else 'Not found.'}\")\n",
        "\n",
        "    # üîº 3. Hypernyms\n",
        "    hypernyms = synsets[0].hypernyms()\n",
        "    if hypernyms:\n",
        "        print(\"üîº Hypernyms:\", ', '.join([h.name().split('.')[0] for h in hypernyms]))\n",
        "    else:\n",
        "        print(\"üîº Hypernyms: Not found.\")\n",
        "\n",
        "    # üîΩ 4. Hyponyms\n",
        "    hyponyms = synsets[0].hyponyms()\n",
        "    if hyponyms:\n",
        "        print(\"üîΩ Hyponyms:\", ', '.join([h.name().split('.')[0] for h in hyponyms[:5]]))  # first 5\n",
        "    else:\n",
        "        print(\"üîΩ Hyponyms: Not found.\")\n",
        "\n",
        "# Run for all words\n",
        "for word in words_to_check:\n",
        "    explore_wordnet(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNr_gEgham8v",
        "outputId": "e172fd4b-cd4f-4409-f9e2-9638e9812540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Word: develop\n",
            "üìò Synonyms: arise, modernise, get, produce, recrudesce, spring_up, grow, originate, train, modernize, prepare, make_grow, educate, rise, germinate, build_up, develop, uprise, explicate, formulate, break, evolve, acquire\n",
            "üìï Antonyms: Not found.\n",
            "üîº Hypernyms: create\n",
            "üîΩ Hyponyms: build\n",
            "\n",
            "üîç Word: technology\n",
            "üìò Synonyms: engineering_science, applied_science, technology, engineering\n",
            "üìï Antonyms: Not found.\n",
            "üîº Hypernyms: profession, application\n",
            "üîΩ Hyponyms: computer_technology, high_technology, aeronautical_engineering, communications_technology, automotive_technology\n",
            "\n",
            "üîç Word: research\n",
            "üìò Synonyms: enquiry, research, search, inquiry, explore\n",
            "üìï Antonyms: Not found.\n",
            "üîº Hypernyms: investigation\n",
            "üîΩ Hyponyms: operations_research, scientific_research, field_work, microscopy, marketing_research\n",
            "\n",
            "üîç Word: language\n",
            "üìò Synonyms: terminology, lyric, linguistic_process, speech_communication, spoken_language, voice_communication, words, nomenclature, speech, oral_communication, language, spoken_communication, linguistic_communication\n",
            "üìï Antonyms: Not found.\n",
            "üîº Hypernyms: communication\n",
            "üîΩ Hyponyms: dead_language, sign_language, string_of_words, object_language, usage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2-ChatGPT:\n",
        "GRAMMATICAL ANALYSIS ‚Äî Detecting Structural Patterns**"
      ],
      "metadata": {
        "id": "wz1aCjVKnHbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the prepared filtered_pos list for pattern detection.\n",
        "# Example target: NN + VB + NN (noun + verb + noun)\n",
        "\n",
        "print(\"[NN + VB + NN] Structures:\")\n",
        "for i in range(len(filtered_pos) - 2):\n",
        "    tag1, tag2, tag3 = filtered_pos[i][1], filtered_pos[i+1][1], filtered_pos[i+2][1]\n",
        "    if tag1.startswith('NN') and tag2.startswith('VB') and tag3.startswith('NN'):\n",
        "        w1, w2, w3 = filtered_pos[i][0], filtered_pos[i+1][0], filtered_pos[i+2][0]\n",
        "        print(f\"‚Üí {w1} ({tag1}) ‚Üí {w2} ({tag2}) ‚Üí {w3} ({tag3})\")\n",
        "\n",
        "print(\"\\n[JJ + NN] Adjective + Noun Structures:\")\n",
        "for i in range(len(filtered_pos) - 1):\n",
        "    tag1, tag2 = filtered_pos[i][1], filtered_pos[i+1][1]\n",
        "    if tag1.startswith('JJ') and tag2.startswith('NN'):\n",
        "        w1, w2 = filtered_pos[i][0], filtered_pos[i+1][0]\n",
        "        print(f\"‚Üí {w1} ({tag1}) ‚Üí {w2} ({tag2})\")\n",
        "\n",
        "print(\"\\n[VB + NN] Verb + Noun Structures:\")\n",
        "for i in range(len(filtered_pos) - 1):\n",
        "    tag1, tag2 = filtered_pos[i][1], filtered_pos[i+1][1]\n",
        "    if tag1.startswith('VB') and tag2.startswith('NN'):\n",
        "        w1, w2 = filtered_pos[i][0], filtered_pos[i+1][0]\n",
        "        print(f\"‚Üí {w1} ({tag1}) ‚Üí {w2} ({tag2})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edrdj9Njm15u",
        "outputId": "a6e35b6a-fd47-4671-9026-fa9366863f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NN + VB + NN] Structures:\n",
            "\n",
            "[JJ + NN] Adjective + Noun Structures:\n",
            "‚Üí (natural (JJ) ‚Üí language (NN)\n",
            "‚Üí steven (JJ) ‚Üí bird (NN)\n",
            "‚Üí new (JJ) ‚Üí york, (NN)\n",
            "‚Üí english, (JJ) ‚Üí spanish, (NN)\n",
            "‚Üí japanese (JJ) ‚Üí text. (NN)\n",
            "\n",
            "[VB + NN] Verb + Noun Structures:\n",
            "‚Üí developed (VBN) ‚Üí university (NN)\n",
            "‚Üí contributed (VBN) ‚Üí development. (NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pattern-based Text Generation ‚Äî Generating Sentences with the Pattern JJ + NN + VB + NN**"
      ],
      "metadata": {
        "id": "Q0V92IVeoWYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "\n",
        "# 1Ô∏è‚É£ Filter words by POS tags\n",
        "adjectives = [word for word, tag in pos_tags if tag.startswith(\"JJ\")]  # Adjectives (e.g., beautiful, large)\n",
        "nouns = [word for word, tag in pos_tags if tag.startswith(\"NN\")]       # Nouns (e.g., book, computer)\n",
        "verbs = [word for word, tag in pos_tags if tag.startswith(\"VB\")]       # Verbs (e.g., run, develop)\n",
        "\n",
        "# 2Ô∏è‚É£ Simple fallback to avoid empty lists\n",
        "if not adjectives: adjectives = [\"great\"]\n",
        "if not nouns: nouns = [\"thing\"]\n",
        "if not verbs: verbs = [\"does\"]\n",
        "\n",
        "# 3Ô∏è‚É£ Sentence generation: create a set number of examples\n",
        "generated_sentences = []\n",
        "for _ in range(5):  # Generate 5 example sentences\n",
        "    adj = random.choice(adjectives)\n",
        "    noun1 = random.choice(nouns)\n",
        "    verb = random.choice(verbs)\n",
        "    noun2 = random.choice(nouns)\n",
        "\n",
        "    # Simple sentence pattern: The [adj] [noun1] [verb]s the [noun2].\n",
        "    sentence = f\"The {adj} {noun1} {verb}s the {noun2}.\"\n",
        "    generated_sentences.append(sentence)\n",
        "\n",
        "# 4Ô∏è‚É£ Print results\n",
        "print(\"\\nüìò Generated Pattern-based Sentences:\")\n",
        "for s in generated_sentences:\n",
        "    print(\"‚Üí\", s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7-ArMi5oKg3",
        "outputId": "411c5262-67b7-4dc6-d980-07dd5b871d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìò Generated Pattern-based Sentences:\n",
            "‚Üí The steven london, useds the bird.\n",
            "‚Üí The english, toolkit) processings the loper..\n",
            "‚Üí The steven development. haves the tokyo.\n",
            "‚Üí The japanese google developeds the google.\n",
            "‚Üí The japanese google iss the pennsylvania.\n"
          ]
        }
      ]
    }
  ]
}